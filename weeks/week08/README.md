# WEEK08 â€” Serving & Scaling Inference

## Goals
- - Serve a model via FastAPI
- - Add simple caching and batching (optional)
- - Try 4-bit quantized inference (optional)
- - Deliverable: `wk08_report.md` + `apps/model_server` updates

## Checklist
- [ ] Reading
- [ ] Experiments
- [ ] Deliverables committed

